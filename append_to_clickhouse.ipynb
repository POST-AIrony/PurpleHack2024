{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch clickhouse_connect accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import logging\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Literal,\n",
    "    Callable,\n",
    "    Iterable,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    cast,\n",
    ")\n",
    "\n",
    "import clickhouse_connect\n",
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def txt2embeddings(text: str, tokenizer, model, device=\"cpu\"):\n",
    "    encoded_input = tokenizer(\n",
    "        [text],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    return mean_pooling(model_output, encoded_input[\"attention_mask\"])[0]\n",
    "\n",
    "\n",
    "def load_models(model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    model = AutoModel.from_pretrained(model)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(BaseModel):\n",
    "    \"\"\"\n",
    "    Модель для представления документа с контентом страницы и метаданными.\n",
    "\n",
    "    Attributes:\n",
    "        page_content (str): Контент страницы документа.\n",
    "        metadata (dict): Метаданные документа. По умолчанию пустой словарь.\n",
    "        type (Literal[\"Document\"]): Тип документа. По умолчанию \"Document\".\n",
    "\n",
    "    Methods:\n",
    "        to_dict: Возвращает словарь с контентом страницы и метаданными.\n",
    "        lc_secrets: Свойство, возвращает пустой словарь для хранения секретных данных.\n",
    "        lc_attributes: Свойство, возвращает пустой словарь для хранения атрибутов.\n",
    "        try_neq_default: Статический метод, проверяет, отличается ли значение от значения по умолчанию для поля модели.\n",
    "\n",
    "    Special Methods:\n",
    "        __repr_args__: Метод для представления аргументов объекта в строке repr.\n",
    "\n",
    "    \"\"\"\n",
    "    page_content: str\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "    type: Literal[\"Document\"] = \"Document\"\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Преобразует документ в словарь.\n",
    "\n",
    "        Returns:\n",
    "            dict: Словарь с контентом страницы и метаданными.\n",
    "        \"\"\"\n",
    "        return {\"page_content\": self.page_content, **self.metadata}\n",
    "\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Возвращает пустой словарь для хранения секретных данных.\n",
    "\n",
    "        Returns:\n",
    "            dict: Пустой словарь.\n",
    "        \"\"\"\n",
    "        return dict()\n",
    "\n",
    "    @property\n",
    "    def lc_attributes(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Возвращает пустой словарь для хранения атрибутов.\n",
    "\n",
    "        Returns:\n",
    "            dict: Пустой словарь.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def try_neq_default(cls, value: Any, key: str, model: BaseModel) -> bool:\n",
    "        \"\"\"\n",
    "        Проверяет, отличается ли значение от значения по умолчанию для поля модели.\n",
    "\n",
    "        Args:\n",
    "            value (Any): Значение поля.\n",
    "            key (str): Название поля.\n",
    "            model (BaseModel): Модель.\n",
    "\n",
    "        Returns:\n",
    "            bool: Результат сравнения.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return model.model_fields[key].default != value\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "    def __repr_args__(self) -> Any:\n",
    "        \"\"\"\n",
    "        Представляет аргументы объекта в строке repr.\n",
    "\n",
    "        Returns:\n",
    "            Any: Аргументы объекта.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (k, v)\n",
    "            for k, v in super().__repr_args__()\n",
    "            if (k not in self.model_fields or self.try_neq_default(v, k, self))\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Класс для токенизации текста.\n",
    "\n",
    "    Attributes:\n",
    "        chunk_overlap (int): Количество символов перекрытия между частями текста.\n",
    "        tokens_per_chunk (int): Максимальное количество токенов в части.\n",
    "        decode (Callable[[List[int]], str]): Функция декодирования списка токенов в строку.\n",
    "        encode (Callable[[str], List[int]]): Функция кодирования строки в список токенов.\n",
    "    \"\"\"\n",
    "    chunk_overlap: int\n",
    "    tokens_per_chunk: int\n",
    "    decode: Callable[[List[int]], str]\n",
    "    encode: Callable[[str], List[int]]\n",
    "\n",
    "\n",
    "class SentenceChunker:\n",
    "    \"\"\"\n",
    "    Класс для разбиения текста на части по предложениям с использованием токенизатора.\n",
    "\n",
    "    Attributes:\n",
    "        chunk_overlap (int): Количество символов перекрытия между частями текста.\n",
    "        model_name (str): Название модели для разбиения текста.\n",
    "        tokens_per_chunk (Optional[int]): Максимальное количество токенов в части.\n",
    "        add_start_index (bool): Флаг добавления индекса начала части.\n",
    "        strip_whitespace (bool): Флаг удаления пробелов в начале и конце части.\n",
    "\n",
    "    Methods:\n",
    "        create_documents(texts: List[str], metadatas: Optional[List[dict]] = None) -> List[Document]:\n",
    "            Создает документы на основе списка текстов.\n",
    "\n",
    "        split_documents(documents: Iterable[Document]) -> List[Any]:\n",
    "            Разбивает список документов на тексты и метаданные и вызывает метод create_documents.\n",
    "\n",
    "        split_text(text: str) -> List[str]:\n",
    "            Разбивает текст на части с помощью токенизатора.\n",
    "\n",
    "        count_tokens(text: str) -> int:\n",
    "            Считает количество токенов в тексте.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_overlap: int = 50,\n",
    "        model_name: str = \"ai-forever/sbert_large_nlu_ru\",\n",
    "        tokens_per_chunk: Optional[int] = None,\n",
    "        add_start_index: bool = False,\n",
    "        strip_whitespace: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModel\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Пожалуйста, установите transformers с помощью `pip install transformers`.\"\n",
    "            )\n",
    "        self._chunk_overlap = chunk_overlap\n",
    "        self.model_name = model_name\n",
    "        self._model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\n",
    "        self._add_start_index = add_start_index\n",
    "        self._strip_whitespace = strip_whitespace\n",
    "\n",
    "    def _initialize_chunk_configuration(\n",
    "        self, *, tokens_per_chunk: Optional[int]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Инициализация конфигурации разбиения текста на части.\n",
    "\n",
    "        Args:\n",
    "            tokens_per_chunk (Optional[int]): Максимальное количество токенов в части.\n",
    "        \"\"\"\n",
    "\n",
    "        # Получение максимального количества токенов модели.\n",
    "        self.maximum_tokens_per_chunk = cast(\n",
    "            int, self._model.config.max_position_embeddings\n",
    "        )\n",
    "\n",
    "        # Установка значения tokens_per_chunk в соответствии с переданным или максимальным значением.\n",
    "        if tokens_per_chunk is None:\n",
    "            self.tokens_per_chunk = self.maximum_tokens_per_chunk\n",
    "        else:\n",
    "            self.tokens_per_chunk = tokens_per_chunk\n",
    "\n",
    "        # Проверка, что tokens_per_chunk не превышает максимальное значение токенов модели.\n",
    "        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\n",
    "            raise ValueError(\n",
    "                f\"Лимит токенов модели '{self.model_name}'\"\n",
    "                f\" составляет: {self.maximum_tokens_per_chunk}.\"\n",
    "                f\" Аргумент tokens_per_chunk={self.tokens_per_chunk}\"\n",
    "                f\" > максимального лимита токенов.\"\n",
    "            )\n",
    "\n",
    "    def create_documents(\n",
    "        self, texts: List[str], metadatas: Optional[List[dict]] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Создание документов на основе текстов.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): Список текстов для обработки.\n",
    "            metadatas (Optional[List[dict]]): Список метаданных для каждого текста.\n",
    "\n",
    "        Returns:\n",
    "            List[Document]: Список документов.\n",
    "        \"\"\"\n",
    "\n",
    "        # Создание пустых списков для текстов и метаданных.\n",
    "        _metadatas = metadatas or [{}] * len(texts)\n",
    "        documents = []\n",
    "\n",
    "        # Итерация по текстам для создания документов.\n",
    "        for i, text in enumerate(texts):\n",
    "            index = 0\n",
    "            previous_chunk_len = 0\n",
    "\n",
    "            # Итерация по частям текста.\n",
    "            for chunk in self.split_text(text):\n",
    "                metadata = copy.deepcopy(_metadatas[i])\n",
    "\n",
    "                # Добавление индекса начала части, если флаг установлен.\n",
    "                if self._add_start_index:\n",
    "                    offset = index + previous_chunk_len - self._chunk_overlap\n",
    "                    index = text.find(chunk, max(0, offset))\n",
    "                    metadata[\"start_index\"] = index\n",
    "                    previous_chunk_len = len(chunk)\n",
    "\n",
    "                # Создание нового документа и добавление его в список документов.\n",
    "                new_doc = Document(page_content=chunk, metadata=metadata)\n",
    "                documents.append(new_doc)\n",
    "        return documents\n",
    "\n",
    "    def split_documents(self, documents: Iterable[Document]) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Разбивает список документов на отдельные тексты и метаданные,\n",
    "        и вызывает метод create_documents для создания документов на основе текстов.\n",
    "\n",
    "        Args:\n",
    "            documents (Iterable[Document]): Итерируемый список документов.\n",
    "\n",
    "        Returns:\n",
    "            List[Any]: Список созданных документов.\n",
    "        \"\"\"\n",
    "        # Извлечение текстов и метаданных из документов\n",
    "        texts, metadatas = [], []\n",
    "        for doc in documents:\n",
    "            texts.append(doc.page_content)\n",
    "            metadatas.append(doc.metadata)\n",
    "        # Создание документов на основе текстов и метаданных\n",
    "        return self.create_documents(texts, metadatas=metadatas)\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разбивает текст на части с помощью токенизатора.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "\n",
    "        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\n",
    "            # Функция для кодирования текста без токенов начала и конца\n",
    "            return self._encode(text)[1:-1]\n",
    "\n",
    "        # Создание экземпляра Tokenizer с необходимыми параметрами\n",
    "        tokenizer = Tokenizer(\n",
    "            chunk_overlap=self._chunk_overlap,\n",
    "            tokens_per_chunk=self.tokens_per_chunk,\n",
    "            decode=self.tokenizer.decode,\n",
    "            encode=encode_strip_start_and_stop_token_ids,\n",
    "        )\n",
    "        # Разбиение текста на части на основе токенов\n",
    "        return self.split_text_on_tokens(text=text, tokenizer=tokenizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разбивает текст на части с учетом токенизатора.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "            tokenizer (Tokenizer): Токенизатор для разбиения текста.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "        # Разбиение текста на части на основе токенов\n",
    "        splits: List[str] = []\n",
    "        input_ids = tokenizer.encode(text)\n",
    "        start_idx = 0\n",
    "        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "        chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        while start_idx < len(input_ids):\n",
    "            splits.append(tokenizer.decode(chunk_ids))\n",
    "            if cur_idx == len(input_ids):\n",
    "                break\n",
    "            start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n",
    "            cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "            chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        return splits\n",
    "\n",
    "    def count_tokens(self, *, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Считает количество токенов в тексте.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "\n",
    "        Returns:\n",
    "            int: Количество токенов.\n",
    "        \"\"\"\n",
    "        return len(self._encode(text))\n",
    "\n",
    "    _max_length_equal_32_bit_integer: int = 2**32\n",
    "\n",
    "    def _encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Кодирует текст в токены с учетом максимальной длины.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: Список токенов.\n",
    "        \"\"\"\n",
    "        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\n",
    "            text,\n",
    "            max_length=self._max_length_equal_32_bit_integer,\n",
    "            truncation=\"do_not_truncate\",\n",
    "        )\n",
    "        return token_ids_with_start_and_end_token_ids\n",
    "\n",
    "\n",
    "class RecursiveChunker:\n",
    "    \"\"\"\n",
    "    Класс для разделения текста на части с учётом различных разделителей и ограничений на размер части.\n",
    "\n",
    "    Args:\n",
    "        separators (Optional[List[str]]): Список разделителей, по которым будет производиться разделение текста.\n",
    "            По умолчанию содержит стандартные разделители: [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
    "        keep_separator (bool): Флаг, определяющий, нужно ли сохранять разделители в итоговых частях текста. По умолчанию True.\n",
    "        is_separator_regex (bool): Флаг, указывающий, являются ли разделители регулярными выражениями. По умолчанию False.\n",
    "        length_function (Callable[[str], int]): Функция для определения длины текста. По умолчанию используется функция len.\n",
    "        chunk_size (int): Максимальный размер части текста в символах. По умолчанию 256.\n",
    "        chunk_overlap (int): Количество символов перекрытия между частями текста. По умолчанию 50.\n",
    "        strip_whitespace (bool): Флаг, определяющий, нужно ли удалять пробелы в начале и конце частей текста. По умолчанию True.\n",
    "        **kwargs: Дополнительные аргументы.\n",
    "\n",
    "    Methods:\n",
    "        _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\n",
    "            Объединяет части текста с разделителем separator, учитывая ограничения на размер части.\n",
    "\n",
    "        _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n",
    "            Объединяет список частей текста в один текст с разделителем separator.\n",
    "\n",
    "        create_documents(self, texts: List[str], metadatas: Optional[List[dict]] = None) -> List[Document]:\n",
    "            Создает список документов на основе текстов.\n",
    "\n",
    "        split_documents(self, documents: Iterable[Document]) -> List[Any]:\n",
    "            Разбивает документы на части и создает новые документы на основе частей текста.\n",
    "\n",
    "        transform_documents(self, documents: Sequence[Document]) -> Sequence[Document]:\n",
    "            Трансформирует документы, разбивая их на части и создавая новые документы на основе частей текста.\n",
    "\n",
    "        _split_text_with_regex(text: str, separator: str, keep_separator: bool) -> List[str]:\n",
    "            Разделяет текст на части с помощью регулярного выражения separator.\n",
    "\n",
    "        _split_text(self, text: str, separators: List[str]) -> List[str]:\n",
    "            Рекурсивно разбивает текст на части с учетом списка разделителей separators.\n",
    "\n",
    "        split_text(self, text: str) -> List[str]:\n",
    "            Разделяет текст на части с учетом установленных разделителей.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        separators: Optional[List[str]] = None,\n",
    "        keep_separator: bool = True,\n",
    "        is_separator_regex: bool = False,\n",
    "        length_function: Callable[[str], int] = len,\n",
    "        chunk_size: int = 256,\n",
    "        chunk_overlap: int = 50,\n",
    "        strip_whitespace: bool = True,\n",
    "        add_start_index: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self._separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        self._is_separator_regex = is_separator_regex\n",
    "        self._chunk_size = chunk_size\n",
    "        self._chunk_overlap = chunk_overlap\n",
    "        self._length_function = length_function\n",
    "        self._keep_separator = keep_separator\n",
    "        self._add_start_index = add_start_index\n",
    "        self._strip_whitespace = strip_whitespace\n",
    "\n",
    "    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Объединяет части текста с разделителем separator, учитывая ограничения на размер части.\n",
    "\n",
    "        Args:\n",
    "            splits (Iterable[str]): Список частей текста.\n",
    "            separator (str): Разделитель для объединения частей.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список объединенных частей текста.\n",
    "        \"\"\"\n",
    "        separator_len = self._length_function(separator)\n",
    "\n",
    "        docs = []\n",
    "        current_doc: List[str] = []\n",
    "        total = 0\n",
    "        for split in splits:\n",
    "            _len = self._length_function(split)\n",
    "            if (\n",
    "                total + _len + (separator_len if len(current_doc) > 0 else 0)\n",
    "                > self._chunk_size\n",
    "            ):\n",
    "                if total > self._chunk_size:\n",
    "                    logger.warning(\n",
    "                        f\"Создан фрагмент размером {total}, \"\n",
    "                        f\"который длиннее указанного {self._chunk_size}\"\n",
    "                    )\n",
    "\n",
    "                if len(current_doc) > 0:\n",
    "                    # Объединяем текущий документ и добавляем его в список объединенных документов.\n",
    "                    doc = self._join_docs(current_doc, separator)\n",
    "                    if doc is not None:\n",
    "                        docs.append(doc)\n",
    "\n",
    "                    # Удаляем из текущего документа части, которые не поместились в текущий чанк.\n",
    "                    while total > self._chunk_overlap or (\n",
    "                        total + _len + (separator_len if len(current_doc) > 0 else 0)\n",
    "                        > self._chunk_size\n",
    "                        and total > 0\n",
    "                    ):\n",
    "                        total -= self._length_function(current_doc[0]) + (\n",
    "                            separator_len if len(current_doc) > 1 else 0\n",
    "                        )\n",
    "                        current_doc = current_doc[1:]\n",
    "            current_doc.append(split)\n",
    "            total += _len + (separator_len if len(current_doc) > 1 else 0)\n",
    "\n",
    "        # Объединяем оставшиеся части текста в последний документ.\n",
    "        doc = self._join_docs(current_doc, separator)\n",
    "        if doc is not None:\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Объединяет список частей текста в один текст с разделителем separator.\n",
    "\n",
    "        Args:\n",
    "            docs (List[str]): Список частей текста.\n",
    "            separator (str): Разделитель для объединения частей.\n",
    "\n",
    "        Returns:\n",
    "            Optional[str]: Объединенный текст или None, если текст пуст.\n",
    "        \"\"\"\n",
    "        text = separator.join(docs)\n",
    "        if self._strip_whitespace:\n",
    "            text = text.strip()\n",
    "        if text == \"\":\n",
    "            return None\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def create_documents(\n",
    "        self, texts: List[str], metadatas: Optional[List[dict]] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Создает список документов на основе текстов.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): Список текстов для обработки.\n",
    "            metadatas (Optional[List[dict]]): Список метаданных для каждого текста.\n",
    "\n",
    "        Returns:\n",
    "            List[Document]: Список созданных документов.\n",
    "        \"\"\"\n",
    "        # Инициализация метаданных для текстов, если они не заданы.\n",
    "        _metadatas = metadatas or [{}] * len(texts)\n",
    "        documents = []\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            index = 0\n",
    "            previous_chunk_len = 0\n",
    "\n",
    "            for chunk in self.split_text(text):\n",
    "                metadata = copy.deepcopy(_metadatas[i])\n",
    "\n",
    "                if self._add_start_index:\n",
    "                    offset = index + previous_chunk_len - self._chunk_overlap\n",
    "                    # Находим индекс начала текущей части в исходном тексте.\n",
    "                    index = text.find(chunk, max(0, offset))\n",
    "                    metadata[\"start_index\"] = index\n",
    "                    previous_chunk_len = len(chunk)\n",
    "\n",
    "                # Создаем новый документ на основе части текста и метаданных.\n",
    "                new_doc = Document(page_content=chunk, metadata=metadata)\n",
    "                documents.append(new_doc)\n",
    "        return documents\n",
    "\n",
    "    def split_documents(self, documents: Iterable[Document]) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Разбивает документы на части и создает новые документы на основе частей текста.\n",
    "\n",
    "        Args:\n",
    "            documents (Iterable[Document]): Список документов для разбиения.\n",
    "\n",
    "        Returns:\n",
    "            List[Any]: Список созданных документов или их частей.\n",
    "        \"\"\"\n",
    "        texts, metadatas = [], []\n",
    "        for doc in documents:\n",
    "            texts.append(doc.page_content)\n",
    "            metadatas.append(doc.metadata)\n",
    "        return self.create_documents(texts, metadatas=metadatas)\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_text_with_regex(\n",
    "        text: str, separator: str, keep_separator: bool\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разделяет текст на части с использованием регулярного выражения separator.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "            separator (str): Регулярное выражение для разделения текста.\n",
    "            keep_separator (bool): Флаг, указывающий, нужно ли сохранять разделители.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "        if separator:\n",
    "            if keep_separator:\n",
    "                splits = re.split(f\"({separator})\", text)\n",
    "                # Объединяем разделители с соответствующими частями текста.\n",
    "                splits = [splits[i] + splits[i + 1] for i in range(1, len(splits), 2)]\n",
    "                # Если количество разделителей нечетное, добавляем последний элемент.\n",
    "                if len(splits) % 2 == 0:\n",
    "                    splits += splits[-1:]\n",
    "                # Добавляем первоначальный элемент текста в начало списка.\n",
    "                splits = [splits[0]] + splits\n",
    "            else:\n",
    "                splits = re.split(separator, text)\n",
    "        else:\n",
    "            # Если разделитель не указан, разбиваем текст на символы.\n",
    "            splits = list(text)\n",
    "        # Удаляем пустые строки из списка.\n",
    "        return [s for s in splits if s != \"\"]\n",
    "\n",
    "    def transform_documents(self, documents: Sequence[Document]) -> Sequence[Document]:\n",
    "        return self.split_documents(list(documents))\n",
    "\n",
    "    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Рекурсивно разбивает текст на части с учетом списка разделителей separators.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "            separators (List[str]): Список разделителей для разбиения текста.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "        # Инициализация пустого списка для хранения окончательных частей текста.\n",
    "        final_chunks = []\n",
    "\n",
    "        # Инициализация разделителя как последнего в списке.\n",
    "        current_separator = separators[-1]\n",
    "\n",
    "        # Инициализация списка разделителей.\n",
    "        remaining_separators = []\n",
    "\n",
    "        # Проверка каждого разделителя в списке.\n",
    "        for i, separator in enumerate(separators):\n",
    "            regex_separator = (\n",
    "                separator if self._is_separator_regex else re.escape(separator)\n",
    "            )\n",
    "\n",
    "            # Если разделитель пустой, устанавливаем его и прерываем цикл.\n",
    "            if separator == \"\":\n",
    "                current_separator = separator\n",
    "                break\n",
    "\n",
    "            # Если найден разделитель в тексте, устанавливаем его и запоминаем оставшиеся разделители.\n",
    "            if re.search(regex_separator, text):\n",
    "                current_separator = separator\n",
    "                remaining_separators = separators[i + 1 :]\n",
    "                break\n",
    "\n",
    "        # Применяем регулярное выражение к разделителю.\n",
    "        regex_separator = (\n",
    "            current_separator\n",
    "            if self._is_separator_regex\n",
    "            else re.escape(current_separator)\n",
    "        )\n",
    "\n",
    "        # Разделяем текст на части с помощью регулярного выражения.\n",
    "        splits = self._split_text_with_regex(\n",
    "            text, regex_separator, self._keep_separator\n",
    "        )\n",
    "\n",
    "        # Создаем список для хранения \"хороших\" частей текста.\n",
    "        good_splits = []\n",
    "\n",
    "        # Инициализация разделителя как пустой строки или исходного разделителя.\n",
    "        current_separator = \"\" if self._keep_separator else current_separator\n",
    "\n",
    "        # Проходим по каждой части текста после разделения.\n",
    "        for split in splits:\n",
    "\n",
    "            # Если длина части меньше максимального размера части, добавляем её в \"хорошие\" части.\n",
    "            if self._length_function(split) < self._chunk_size:\n",
    "                good_splits.append(split)\n",
    "            else:\n",
    "\n",
    "                # Если часть больше максимального размера, объединяем \"хорошие\" части и добавляем в итоговый список.\n",
    "                if good_splits:\n",
    "                    merged_text = self._merge_splits(good_splits, current_separator)\n",
    "                    final_chunks.extend(merged_text)\n",
    "                    good_splits = []\n",
    "\n",
    "                # Если есть новые разделители, разбиваем текущую часть на еще более мелкие части.\n",
    "                if not remaining_separators:\n",
    "                    final_chunks.append(split)\n",
    "                else:\n",
    "                    other_chunks = self._split_text(split, remaining_separators)\n",
    "                    final_chunks.extend(other_chunks)\n",
    "\n",
    "        # Добавляем \"хорошие\" части, если они остались после обработки.\n",
    "        if good_splits:\n",
    "            merged_text = self._merge_splits(good_splits, current_separator)\n",
    "            final_chunks.extend(merged_text)\n",
    "\n",
    "        # Возвращаем итоговый список частей текста.\n",
    "        return final_chunks\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разделяет текст на части с учетом установленных разделителей.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "        return self._split_text(text, self._separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"SbertEmb\"\n",
    "MODEL_EMB_NAME = \"ai-forever/sbert_large_nlu_ru\"\n",
    "HOST = \"716c-81-5-106-50.ngrok-free.app\"\n",
    "PORT = \"80\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"От Об отмене Методических рекомендаций Банка России по усилению контроля операторами по переводу денежных средств за деятельностью банковских платежных агентов от 14 04 2016 11-МР \n",
    "С даты издания настоящего письма Банка России отменяются Методические рекомендации Банка России по усилению контроля операторами по переводу денежных средств за деятельностью банковских платежных агентов от 14 04 2016 11-МР \n",
    "Настоящее письмо Банка России подлежит опубликованию в Вестнике Банка России и размещению на официальном сайте Банка России в информационно-телекоммуникационной сети Интернет \n",
    "Первый заместитель Председателя Банка России О Н Скоробогатова\"\"\"   #String\n",
    "title = \"Об отмене Методических рекомендаций Банка России по усилению контроля операторами по переводу денежных средств за деятельностью банковских платежных агентов от 14.04.2016 11-МР\"  #String\n",
    "url = \"https://cbr.ru/Crosscut/LawActs/File/7673\"   #String\n",
    "date = \"от 11.01.2024\"  #String\n",
    "number = \"04-45/107\"    #String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_models(MODEL_EMB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = txt2embeddings(text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ping: True\n"
     ]
    }
   ],
   "source": [
    "client = clickhouse_connect.get_client(host=HOST, port=PORT)\n",
    "print(\"Ping:\", client.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.summary.QuerySummary at 0x75614c8c8850>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = \",\".join([str(float(i)) for i in embeddings])\n",
    "values = f\"('{title}','{url}','{date}','{number}','{text}','[{vectors}]'),\"\n",
    "query = f\"\"\"INSERT INTO \"{TABLE_NAME}\"(\"Name\",\"Url\",\"Date\",\"Number\", \"Text\", \"Embedding\") VALUES {values}\"\"\"\n",
    "client.command(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Purple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
